{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyvpJ-AGKg2f"
   },
   "source": [
    "# Airline Tweets Sentiment Analysis\n",
    "\n",
    "This analysis classifies airline tweets as having a positive or negative sentiment, based on the words that are used within the Tweets. A deep learning model will be trained on the pre-processed textual Tweet data in order to predict sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 915
    },
    "id": "Y62v7gxKppBK",
    "outputId": "00bded20-4460-436a-bca6-0800d7e02479"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>569587686496825344</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KristenReenders</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 12:01:01 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>569587371693355008</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>itsropes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:46 -0800</td>\n",
       "      <td>Texas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>569587242672398336</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sanyabun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:15 -0800</td>\n",
       "      <td>Nigeria,lagos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>569587188687634433</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>0.6659</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SraJackson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:02 -0800</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>569587140490866689</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>daviddtwu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:58:51 -0800</td>\n",
       "      <td>dallas, TX</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id  ...               user_timezone\n",
       "0      570306133677760513  ...  Eastern Time (US & Canada)\n",
       "1      570301130888122368  ...  Pacific Time (US & Canada)\n",
       "2      570301083672813571  ...  Central Time (US & Canada)\n",
       "3      570301031407624196  ...  Pacific Time (US & Canada)\n",
       "4      570300817074462722  ...  Pacific Time (US & Canada)\n",
       "...                   ...  ...                         ...\n",
       "14635  569587686496825344  ...                         NaN\n",
       "14636  569587371693355008  ...                         NaN\n",
       "14637  569587242672398336  ...                         NaN\n",
       "14638  569587188687634433  ...  Eastern Time (US & Canada)\n",
       "14639  569587140490866689  ...                         NaN\n",
       "\n",
       "[14640 rows x 15 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# import CSV file from GitHub\n",
    "data_url = 'https://raw.githubusercontent.com/msda665/MSDA665/main/Tweets.csv'\n",
    "\n",
    "# save data as dataframe\n",
    "dat = pd.read_csv(data_url)\n",
    "\n",
    "# display data\n",
    "display(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tqW6ZA3ushYC",
    "outputId": "d031ed02-f285-4a8c-9511-00068ab6c599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
      "       'negativereason', 'negativereason_confidence', 'airline',\n",
      "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
      "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
      "       'tweet_location', 'user_timezone'],\n",
      "      dtype='object')\n",
      "\n",
      " ['neutral' 'positive' 'negative']\n"
     ]
    }
   ],
   "source": [
    "# verify column names\n",
    "print(dat.columns)\n",
    "\n",
    "# verify the number of unique sentiments for Airline Tweets\n",
    "print('\\n', dat['airline_sentiment'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ODE17OFtVcy"
   },
   "source": [
    "## Initial processing\n",
    "\n",
    "The data set contains 3 sentiments: neutral, positive, and negative. Since the objective is to classify positive or negative sentiments, any Tweets with a 'neutral' sentiment will be removed from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJ_iz3gzuRPK",
    "outputId": "0a244eff-ed55-4412-cc96-79dc989444dc",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        False\n",
      "1         True\n",
      "2        False\n",
      "3         True\n",
      "4         True\n",
      "         ...  \n",
      "14635     True\n",
      "14636     True\n",
      "14637    False\n",
      "14638     True\n",
      "14639    False\n",
      "Name: airline_sentiment, Length: 14640, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "is_neutral = dat['airline_sentiment'] != 'neutral'\n",
    "print(is_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2H-Kf0i5P7X",
    "outputId": "6cb3263c-8fa5-4d74-cb94-6cb8c37e3bdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new dataframe with only 'positive' and 'negative' sentiments\n",
    "dat2 = dat[is_neutral]\n",
    "\n",
    "# verify the unique sentiments in the new dataframe\n",
    "dat2['airline_sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37cIS7QixQEB"
   },
   "source": [
    "## Further processing\n",
    "\n",
    "Since the input variable is 'text', and the target variable is 'airline_sentiment', only these 2 columns will be retained in the data set.\n",
    "\n",
    "Additionally, the sentiment column will be transformed, such that positive sentiments are represented by a 1, and negative sentiments are represented by a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HEJFNZAxSX6C",
    "outputId": "566b3030-67f9-4e6c-d6bd-2e9b3476c084"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['airline_sentiment', 'text'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new dataframe with only the sentiment and text columns\n",
    "df = dat2.loc[:, ['airline_sentiment', 'text']]\n",
    "\n",
    "# verify the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LzPTrUYx8MWj",
    "outputId": "fc00a683-bc63-4350-8c81-44e4c1df662c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1         True\n",
      "3        False\n",
      "4        False\n",
      "5        False\n",
      "6         True\n",
      "         ...  \n",
      "14633    False\n",
      "14634    False\n",
      "14635     True\n",
      "14636    False\n",
      "14638    False\n",
      "Name: airline_sentiment, Length: 11541, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# store only the sentiment column\n",
    "senti = df['airline_sentiment']\n",
    "\n",
    "# return positive sentiments\n",
    "print(senti == 'positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "WfbmyZh79OOc",
    "outputId": "098a8f0c-57aa-4aed-fd8f-b7b5936d1263"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14633</th>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir my flight was Cancelled Flightled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14634</th>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir right on cue with the delaysðŸ‘Œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>1</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11541 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      airline_sentiment                                               text\n",
       "1                     1  @VirginAmerica plus you've added commercials t...\n",
       "3                     0  @VirginAmerica it's really aggressive to blast...\n",
       "4                     0  @VirginAmerica and it's a really big bad thing...\n",
       "5                     0  @VirginAmerica seriously would pay $30 a fligh...\n",
       "6                     1  @VirginAmerica yes, nearly every time I fly VX...\n",
       "...                 ...                                                ...\n",
       "14633                 0  @AmericanAir my flight was Cancelled Flightled...\n",
       "14634                 0         @AmericanAir right on cue with the delaysðŸ‘Œ\n",
       "14635                 1  @AmericanAir thank you we got on a different f...\n",
       "14636                 0  @AmericanAir leaving over 20 minutes Late Flig...\n",
       "14638                 0  @AmericanAir you have my money, you change my ...\n",
       "\n",
       "[11541 rows x 2 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# replace all positive sentiments with 1 and all negative sentiments with 0\n",
    "senti[senti == 'positive'] = 1\n",
    "senti[senti == 'negative'] = 0\n",
    "\n",
    "# assign numeric sentiments back to df\n",
    "df['airline_sentiment'] = senti\n",
    "\n",
    "# verify the data\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3A3vfdWA0i7"
   },
   "source": [
    "## Partition data\n",
    "\n",
    "The data set will be partitioned into a training and test set, where 2/3 of the observations are used for training, and the remaining 1/3 of observations are used for testing the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ynXmdSGqwu3o",
    "outputId": "57d1db3f-3813-466f-baa2-0c453f9b2988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11541\n"
     ]
    }
   ],
   "source": [
    "# store sentiments as a list\n",
    "labels = list(df['airline_sentiment'])\n",
    "\n",
    "# store the number of labels\n",
    "m = len(labels)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2j4mBrq--w8",
    "outputId": "e8e81d22-8ce1-4450-b7f0-a69b3f5e0f33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set length:  7698\n",
      "Test data set length:  3843\n"
     ]
    }
   ],
   "source": [
    "# store Tweet text as a list\n",
    "tweets = list(df['text'])\n",
    "\n",
    "# partition data into training and test sets\n",
    "# 2/3 of observations in training set\n",
    "# 1/3 of observations in test set\n",
    "train_labels = labels[:7698]\n",
    "test_labels = labels[7698:]\n",
    "train_tweets = tweets[:7698]\n",
    "test_tweets = tweets[7698:]\n",
    "\n",
    "# verify number of observations\n",
    "print('Training data set length: ', len(train_labels))\n",
    "print('Test data set length: ', len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8_fIOrxgAXJt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# convert labels into np arrays for Neural Network processing in Tensorflow\n",
    "train_labels_final = np.array(train_labels)\n",
    "test_labels_final = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlWuoL56L440"
   },
   "source": [
    "## Deep learning model\n",
    "\n",
    "The Tensorflow package will be used to train a neural network model. Since neural networks require numeric input, each Tweet will be converted from a string to a vector by building a word index. Each word in each vector is assigned an integer, which represents the location of that word in the word index. Each vector will contain 120 numbers. Vectors which are longer than 120 will be truncated by removing the ending numbers past 120, and any vectors which contain less than 120 numbers will be padded by placing zeros at the beginning of the vector (before the word index values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "v59byeJfL33M"
   },
   "outputs": [],
   "source": [
    "# declare deep learning hyperparameters\n",
    "\n",
    "# vocabulary of 10,000 words\n",
    "vocab_size = 10000\n",
    "\n",
    "# Tweet/vector lengths of 120 words\n",
    "max_length = 120\n",
    "\n",
    "# truncate words at the end of the Tweet if the Tweet has more than 120 words\n",
    "trunc_type='post'\n",
    "\n",
    "# specify the Out of Vocab token value\n",
    "oov_tok = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JrXrnTwWMHrs"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# initialize tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "\n",
    "# fit tokenizer to the training Tweets\n",
    "tokenizer.fit_on_texts(train_tweets)\n",
    "\n",
    "# create word index from fitted tokenizer\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# convert training Tweet strings into vectors\n",
    "# tokens are represented as integers, based on token's index value in word_index\n",
    "sequences = tokenizer.texts_to_sequences(train_tweets)\n",
    "\n",
    "# pad each vector with zeros (0), so that all vector lengths = 120\n",
    "# zeros are pre-padded by default\n",
    "train_padded = pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=max_length,\n",
    "    truncating=trunc_type\n",
    ")\n",
    "\n",
    "# convert test Tweet strings into vectors\n",
    "test_sequences = tokenizer.texts_to_sequences(test_tweets)\n",
    "\n",
    "# pad the test vectors\n",
    "test_padded = pad_sequences(\n",
    "    test_sequences,\n",
    "    maxlen=max_length,\n",
    "    truncating=trunc_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vRjh6t8fNXiw",
    "outputId": "d7978aca-bdd7-485c-c05e-4893d2bf2ecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index:  [('<OOV>', 1), ('to', 2), ('the', 3), ('united', 4)]\n",
      "Reverse word index:  [(1, '<OOV>'), (2, 'to'), (3, 'the'), (4, 'united')]\n"
     ]
    }
   ],
   "source": [
    "# create word index with the index as key, word as value\n",
    "# needed for converting vectors back into strings\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_index.items()]\n",
    ")\n",
    "\n",
    "# verify the original and reversed word_indexes\n",
    "print('Word index: ', list(word_index.items())[:4])\n",
    "print('Reverse word index: ', list(reverse_word_index.items())[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41gdpiPWN66W",
    "outputId": "ea2f8158-9261-4be5-d517-8688fbb9fe46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded vector:  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    4\n",
      "   16    3  743   70  107 1968  313   77  120 2814  284   16  118 3630\n",
      "  910 1518    8 1796    9   80   32  138]\n",
      "Decoded Tweet:  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? united in the future when delay causes 15 hour wait slept night in airport ensuring seating choice for replacement flight would be good\n"
     ]
    }
   ],
   "source": [
    "# create function which converts vectors into strings\n",
    "def decode_review(seq):\n",
    "\n",
    "  # replace padded zeros with a question mark\n",
    "  return ' '.join([reverse_word_index.get(i, '?') for i in seq])\n",
    "\n",
    "# print a sample Tweet and its padded version\n",
    "print('Padded vector: ', train_padded[1234])\n",
    "print('Decoded Tweet: ', decode_review(train_padded[1234]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDcRkqW_OLxP"
   },
   "source": [
    "## Neural network configuration\n",
    "\n",
    "The neural network is built using the Tensorflow package. The following layers are used within the neural network:\n",
    "1. Embedding layer, so that vectors with similar words are numerically closer\n",
    "2. Flattening layer, to transform each vector into 1 dimension\n",
    "3. Hidden layer, with 6 nodes and ReLu used as the activation function (output is any positive number)\n",
    "4. Output layer, with 1 node to represent the probability that the Tweet has a positive sentiment. The Sigmoid activation function is chosen so that the probability ranges from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-Jz2wwuOJyU",
    "outputId": "a36ed52b-613a-4d85-e12b-1efb1c782aa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define embedding dimension\n",
    "# so vectors with similar words are numerically closer\n",
    "embedding_dimension = 16\n",
    "\n",
    "# initialize neural network\n",
    "neural_net = tf.keras.Sequential([\n",
    "      \n",
    "      # embedding layer as first layer\n",
    "      tf.keras.layers.Embedding(\n",
    "          vocab_size,\n",
    "          embedding_dimension,\n",
    "          input_length=max_length\n",
    "      ),\n",
    "\n",
    "      # flatten layer to flatten each vector into 1 dimension\n",
    "      tf.keras.layers.Flatten(),\n",
    "\n",
    "      # hidden layer with 6 nodes and ReLu as activation function\n",
    "      # output is any positive real number\n",
    "      tf.keras.layers.Dense(6, activation='relu'),\n",
    "\n",
    "      # output layer with 1 node to represent the probability of the Tweet being positive sentiment\n",
    "      # Sigmoid activation function so probability ranges from 0 to 1\n",
    "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# declare optimizer, loss function, and validation metric\n",
    "neural_net.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# summarize the neural network\n",
    "neural_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPtZW3vLOzla"
   },
   "source": [
    "## Model training\n",
    "\n",
    "The neural network is trained using 10 epoch iterations. The accuracy of the model classification is chosen as the validation metric for model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "896t0CibOu11",
    "outputId": "1adc46d3-509c-4ef6-90ac-65ed5d511176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "241/241 [==============================] - 2s 5ms/step - loss: 0.4693 - accuracy: 0.7822 - val_loss: 0.2693 - val_accuracy: 0.8967\n",
      "Epoch 2/10\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.2637 - accuracy: 0.9041 - val_loss: 0.1780 - val_accuracy: 0.9303\n",
      "Epoch 3/10\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.1245 - accuracy: 0.9583 - val_loss: 0.1683 - val_accuracy: 0.9373\n",
      "Epoch 4/10\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0684 - accuracy: 0.9800 - val_loss: 0.1740 - val_accuracy: 0.9365\n",
      "Epoch 5/10\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0386 - accuracy: 0.9906 - val_loss: 0.1870 - val_accuracy: 0.9357\n",
      "Epoch 6/10\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0228 - accuracy: 0.9961 - val_loss: 0.2047 - val_accuracy: 0.9352\n",
      "Epoch 7/10\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0142 - accuracy: 0.9977 - val_loss: 0.2224 - val_accuracy: 0.9329\n",
      "Epoch 8/10\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0090 - accuracy: 0.9992 - val_loss: 0.2316 - val_accuracy: 0.9347\n",
      "Epoch 9/10\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0062 - accuracy: 0.9997 - val_loss: 0.2473 - val_accuracy: 0.9329\n",
      "Epoch 10/10\n",
      "241/241 [==============================] - 1s 5ms/step - loss: 0.0046 - accuracy: 0.9997 - val_loss: 0.2579 - val_accuracy: 0.9310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6cd405ed10>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the neural network using 10 epoch iterations\n",
    "# epoch = 1 cycle of forward-feeding and backpropagation\n",
    "num_epochs = 10\n",
    "\n",
    "# train the model with padded training Tweets\n",
    "neural_net.fit(\n",
    "    train_padded,\n",
    "    train_labels_final,\n",
    "    epochs=num_epochs,\n",
    "\n",
    "    # validate the model using the test Tweets\n",
    "    validation_data=(test_padded, test_labels_final)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDQw4K61YRbt"
   },
   "source": [
    "## Model evaluation\n",
    "\n",
    "After 10 epochs of training, the model classified approximately 99% of the training observations correctly, whereas the model classified 90% of the testing observations correctly.\n",
    "\n",
    "The next step is to predict the sentiment of new Tweets using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdrqZQxMJa5N",
    "outputId": "000202cf-59d4-414d-ed3c-4b7b5553a153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of positive sentiment for first Tweet:  [99.90274] %\n",
      "Probability of positive sentiment for second Tweet:  [0.00546293] %\n"
     ]
    }
   ],
   "source": [
    "# test neural network using new Tweets\n",
    "my_tweets = [\n",
    "             '@AmericanAirlines I greatly enjoyed the flight. The beverage selection was excellent.', \n",
    "             '@AmericanAirlines This flight was a waste of money. It arrived late at the destination.',\n",
    "]\n",
    "\n",
    "# tokenize, convert to vector, then pad the Tweets\n",
    "my_sequences = tokenizer.texts_to_sequences(my_tweets)\n",
    "\n",
    "my_padded = pad_sequences(\n",
    "    my_sequences,\n",
    "    maxlen=max_length,\n",
    "    truncating=trunc_type\n",
    ")\n",
    "\n",
    "# predict the sentiment of the new Tweets using the trained model\n",
    "predictions = neural_net.predict(my_padded)\n",
    "\n",
    "print('Probability of positive sentiment for first Tweet: ', predictions[0] * 100, '%')\n",
    "print('Probability of positive sentiment for second Tweet: ', predictions[1] * 100, '%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework 7 Aaron Bachman.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
